{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573d9e8-ffe2-4348-b068-c17e43e88257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb19c4-348d-4f8e-bd59-825cc97129b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c46ded7-5b0f-46ef-b26a-7526ab2e4d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import kafka\n",
    "print(kafka.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7f0d52-a235-4692-b29b-d7c1459b0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f46af6-4fd1-47de-ad09-07ff8ae253ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef7f5f-825a-4d84-8314-30cad405759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c283a0-3302-4941-8dbe-a445e8051404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at ../../data/sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [24, 27, 22, 32, 29],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "base_path = \"../../data/\"\n",
    "csv_name = \"sample_data.csv\"\n",
    "csv_path = base_path + csv_name\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file created at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db51d36-28e3-4757-af82-e5c99c472d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id     name  age         city\n",
      "0   1    Alice   24     New York\n",
      "1   2      Bob   27  Los Angeles\n",
      "2   3  Charlie   22      Chicago\n",
      "3   4    David   32      Houston\n",
      "4   5      Eve   29      Phoenix\n"
     ]
    }
   ],
   "source": [
    "# Read the created CSV file to verify\n",
    "df_check = pd.read_csv(csv_path)\n",
    "print(df_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f950fcb-013e-4d6a-aec8-ac7e828fbd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 22:06:26 WARN Utils: Your hostname, MacBook-Pro-de-Victor.local resolves to a loopback address: 127.0.0.1; using 192.168.1.130 instead (on interface en0)\n",
      "24/11/10 22:06:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/victorgalan/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/victorgalan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/victorgalan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4064855-c4e1-4f36-a318-07c3451c2370;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1191ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   1   |   1   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4064855-c4e1-4f36-a318-07c3451c2370\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/10ms)\n",
      "24/11/10 22:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Kafka\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e58df5-e6b9-496e-af19-fd3742b643b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configurations\n",
    "\n",
    "# Update with your Kafka bootstrap servers\n",
    "kafka_bootstrap_servers = \"localhost:9093, localhost:9095, localhost:9097\" \n",
    "\n",
    "# Replace with your Kafka topic\n",
    "kafka_topic = \"test2-topic\"            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed6486-8b5b-4674-8431-d948ab17a1ab",
   "metadata": {},
   "source": [
    "# 1st approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b95cace-801d-479d-9d6f-5555f62efc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read CSV file\n",
    "# Replace csv_path with the path to your CSV file\n",
    "df = spark.read.option(\"header\", \"true\").csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86cde571-7d80-4984-9fe1-dd72515a2294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+\n",
      "| id|   name|age|       city|\n",
      "+---+-------+---+-----------+\n",
      "|  1|  Alice| 24|   New York|\n",
      "|  2|    Bob| 27|Los Angeles|\n",
      "|  3|Charlie| 22|    Chicago|\n",
      "|  4|  David| 32|    Houston|\n",
      "|  5|    Eve| 29|    Phoenix|\n",
      "+---+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad58fa-9579-47b2-9476-f7c951ce6be3",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for Kafka by selecting columns as 'key' and 'value'\n",
    " Kafka expects 'key' and 'value' columns as byte array, so we need to format it accordingly.\n",
    " In this example, we're serializing the entire row as a JSON string and setting it as the value.\n",
    " If you have a specific column for key, you can use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27aff6b5-0bd8-457c-b6e3-203ac1265a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we structure all columns as JSON and assign it to the 'value' column\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"value\", \n",
    "        to_json(struct([df[x] for x in df.columns])))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99097945-428d-4c28-9a89-d25b936bffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+-------------------------------------------------------+\n",
      "|id |name   |age|city       |value                                                  |\n",
      "+---+-------+---+-----------+-------------------------------------------------------+\n",
      "|1  |Alice  |24 |New York   |{\"id\":\"1\",\"name\":\"Alice\",\"age\":\"24\",\"city\":\"New York\"} |\n",
      "|2  |Bob    |27 |Los Angeles|{\"id\":\"2\",\"name\":\"Bob\",\"age\":\"27\",\"city\":\"Los Angeles\"}|\n",
      "|3  |Charlie|22 |Chicago    |{\"id\":\"3\",\"name\":\"Charlie\",\"age\":\"22\",\"city\":\"Chicago\"}|\n",
      "|4  |David  |32 |Houston    |{\"id\":\"4\",\"name\":\"David\",\"age\":\"32\",\"city\":\"Houston\"}  |\n",
      "|5  |Eve    |29 |Phoenix    |{\"id\":\"5\",\"name\":\"Eve\",\"age\":\"29\",\"city\":\"Phoenix\"}    |\n",
      "+---+-------+---+-----------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8059ebb-43c1-45ef-a43a-038a79c1f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebb883-ee41-42a8-9a35-151cac59665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6d931-7067-4699-a3f2-4bb50b6f45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1e848-07f4-4cf6-95c2-0eccb14a8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24045405-2b8b-4093-bb3f-179f5cd1a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa4867-6ef5-4808-845e-e5693389c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema explicitly for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the streaming CSV data with the defined schema\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(base_path)\n",
    "\n",
    "# Check the schema of the streaming DataFrame\n",
    "streaming_df.printSchema()\n",
    "\n",
    "# Proceed with processing the streaming DataFrame, for example:\n",
    "# You can write the data to Kafka or perform other transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1a4af-8cbb-4d6b-a455-ba48891dee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to send batch data to Kafka\n",
    "def send_batch_to_kafka(batch_df, batch_id):\n",
    "    rows = batch_df.collect()  # Collect the rows of the batch\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=kafka_bootstrap_servers,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    for row in rows:\n",
    "        data = {'value': row.asDict()}\n",
    "        producer.send(kafka_topic, value=data)\n",
    "    producer.flush()\n",
    "\n",
    "# Apply foreachBatch for streaming DataFrame\n",
    "query = streaming_df.writeStream \\\n",
    "    .foreachBatch(send_batch_to_kafka) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd881f45-df66-4ff1-b7e5-820c833dcb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c4d200-9706-43f7-8b11-b158d71d2a42",
   "metadata": {},
   "source": [
    "# 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7290bbd-80ba-42b9-8fe5-063ee4677b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=kafka_bootstrap_servers,\n",
    "    value_serializer=lambda v: v.encode('utf-8')  # Encode messages as UTF-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ae872cb-3add-401f-9747-bb43cecfa2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafka.producer.kafka.KafkaProducer"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c066bf5-3c71-49df-b7f9-10574e024901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: 1,Alice,24,New York\n",
      "Sent message: 2,Bob,27,Los Angeles\n",
      "Sent message: 3,Charlie,22,Chicago\n",
      "Sent message: 4,David,32,Houston\n",
      "Sent message: 5,Eve,29,Phoenix\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file as a regular text file\n",
    "with open(csv_path, mode='r') as file:\n",
    "    # Read and skip the header if needed\n",
    "    header = next(file)\n",
    "\n",
    "    # Send each line as a message\n",
    "    for line in file:\n",
    "        # Strip any newline characters from the line\n",
    "        message = line.strip()\n",
    "        \n",
    "        # Send the message to Kafka\n",
    "        producer.send(kafka_topic, value=message)\n",
    "        print(f\"Sent message: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48639721-b007-4cff-86f6-755a24e54501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the producer connection\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986671e-5e39-48b3-ba21-1d62e8af356f",
   "metadata": {},
   "source": [
    "# 3rd approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7cc918d-faa2-4e49-8093-55733933e169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: {'value': {'id': '1', 'name': 'Alice', 'age': '24', 'city': 'New York'}}\n",
      "Sent message: {'value': {'id': '2', 'name': 'Bob', 'age': '27', 'city': 'Los Angeles'}}\n",
      "Sent message: {'value': {'id': '3', 'name': 'Charlie', 'age': '22', 'city': 'Chicago'}}\n",
      "Sent message: {'value': {'id': '4', 'name': 'David', 'age': '32', 'city': 'Houston'}}\n",
      "Sent message: {'value': {'id': '5', 'name': 'Eve', 'age': '29', 'city': 'Phoenix'}}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize Kafka producer with JSON serialization\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=kafka_bootstrap_servers,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "\n",
    "# Open the CSV file and read each row\n",
    "with open(csv_path, mode='r') as file:\n",
    "    csv_reader = csv.DictReader(file)  # Automatically reads the header\n",
    "\n",
    "    for row in csv_reader:\n",
    "        # Construct the message as per the required JSON structure\n",
    "        message = {\"value\": row}\n",
    "\n",
    "        # Send the message to Kafka\n",
    "        producer.send(kafka_topic, value=message)\n",
    "        print(f\"Sent message: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed9bb8a5-182d-4d3a-8201-a322fc634132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the producer connection\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbe913-ce8a-43ae-ba6e-a4124e12bbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "060722c3-b065-4b9d-bc90-c33f06020176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc12bd-fd43-4182-a0f1-213eb3674cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iceberg_env)",
   "language": "python",
   "name": "iceberg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
